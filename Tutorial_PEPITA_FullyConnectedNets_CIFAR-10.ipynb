{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0787a534",
   "metadata": {},
   "source": [
    "This notebook illustrates how to train Fully Connected models with PEPITA. We train and test the model on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a36dc",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56a3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6701be",
   "metadata": {},
   "source": [
    "#### Define Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d192b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models with Dropout\n",
    "class NetFC1x1024DOcust(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3,1024,bias=False)\n",
    "        self.fc2 = nn.Linear(1024, 10,bias=False)\n",
    "        \n",
    "        # initialize the layers using the He uniform initialization scheme\n",
    "        fc1_nin = 32*32*3 # Note: if dataset is MNIST --> fc1_nin = 28*28*1\n",
    "        fc1_limit = np.sqrt(6.0 / fc1_nin)\n",
    "        torch.nn.init.uniform_(self.fc1.weight, a=-fc1_limit, b=fc1_limit)\n",
    "        fc2_nin = 1024\n",
    "        fc2_limit = np.sqrt(6.0 / fc2_nin)\n",
    "        torch.nn.init.uniform_(self.fc2.weight, a=-fc2_limit, b=fc2_limit)\n",
    "        \n",
    "\n",
    "    def forward(self, x, do_masks):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # apply dropout --> we use a custom dropout implementation because we need to present the same dropout mask in the two forward passes\n",
    "        if do_masks is not None:\n",
    "            x = x * do_masks[0]   \n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad144c5",
   "metadata": {},
   "source": [
    "#### Set hyperparameters and train+test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2a085cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "norm of w at layer 0 is 45.241451263427734\n",
      "norm of w at layer 1 is 4.473217010498047\n",
      "torch.Size([3072, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\giorgiadellaferrera\\anaconda3\\envs\\env_pytorch\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 2.210\n",
      "Testing...\n",
      "Test accuracy epoch 0: 35.12 %\n",
      "[2,   500] loss: 2.167\n",
      "Testing...\n",
      "Test accuracy epoch 1: 38.07 %\n",
      "[3,   500] loss: 2.150\n",
      "Testing...\n",
      "Test accuracy epoch 2: 40.9 %\n",
      "[4,   500] loss: 2.137\n",
      "Testing...\n",
      "Test accuracy epoch 3: 40.41 %\n",
      "[5,   500] loss: 2.126\n",
      "Testing...\n",
      "Test accuracy epoch 4: 41.4 %\n",
      "[6,   500] loss: 2.117\n",
      "Testing...\n",
      "Test accuracy epoch 5: 39.76 %\n",
      "[7,   500] loss: 2.109\n",
      "Testing...\n",
      "Test accuracy epoch 6: 43.73 %\n",
      "[8,   500] loss: 2.101\n",
      "Testing...\n",
      "Test accuracy epoch 7: 45.08 %\n",
      "[9,   500] loss: 2.096\n",
      "Testing...\n",
      "Test accuracy epoch 8: 45.28 %\n",
      "[10,   500] loss: 2.090\n",
      "Testing...\n",
      "Test accuracy epoch 9: 45.05 %\n",
      "[11,   500] loss: 2.085\n",
      "Testing...\n",
      "Test accuracy epoch 10: 44.66 %\n",
      "[12,   500] loss: 2.079\n",
      "Testing...\n",
      "Test accuracy epoch 11: 45.89 %\n",
      "[13,   500] loss: 2.074\n",
      "Testing...\n",
      "Test accuracy epoch 12: 47.39 %\n",
      "[14,   500] loss: 2.071\n",
      "Testing...\n",
      "Test accuracy epoch 13: 46.95 %\n",
      "[15,   500] loss: 2.065\n",
      "Testing...\n",
      "Test accuracy epoch 14: 47.41 %\n",
      "[16,   500] loss: 2.061\n",
      "Testing...\n",
      "Test accuracy epoch 15: 47.9 %\n",
      "[17,   500] loss: 2.056\n",
      "Testing...\n",
      "Test accuracy epoch 16: 48.52 %\n",
      "[18,   500] loss: 2.052\n",
      "Testing...\n",
      "Test accuracy epoch 17: 48.53 %\n",
      "[19,   500] loss: 2.048\n",
      "Testing...\n",
      "Test accuracy epoch 18: 46.39 %\n",
      "[20,   500] loss: 2.042\n",
      "Testing...\n",
      "Test accuracy epoch 19: 48.61 %\n",
      "[21,   500] loss: 2.040\n",
      "Testing...\n",
      "Test accuracy epoch 20: 47.27 %\n",
      "[22,   500] loss: 2.034\n",
      "Testing...\n",
      "Test accuracy epoch 21: 49.05 %\n",
      "[23,   500] loss: 2.034\n",
      "Testing...\n",
      "Test accuracy epoch 22: 49.2 %\n",
      "[24,   500] loss: 2.030\n",
      "Testing...\n",
      "Test accuracy epoch 23: 49.39 %\n",
      "[25,   500] loss: 2.027\n",
      "Testing...\n",
      "Test accuracy epoch 24: 48.68 %\n",
      "[26,   500] loss: 2.021\n",
      "Testing...\n",
      "Test accuracy epoch 25: 49.64 %\n",
      "[27,   500] loss: 2.019\n",
      "Testing...\n",
      "Test accuracy epoch 26: 49.09 %\n",
      "[28,   500] loss: 2.015\n",
      "Testing...\n",
      "Test accuracy epoch 27: 50.06 %\n",
      "[29,   500] loss: 2.010\n",
      "Testing...\n",
      "Test accuracy epoch 28: 49.31 %\n",
      "[30,   500] loss: 2.009\n",
      "Testing...\n",
      "Test accuracy epoch 29: 49.77 %\n",
      "[31,   500] loss: 2.005\n",
      "Testing...\n",
      "Test accuracy epoch 30: 49.89 %\n",
      "[32,   500] loss: 1.999\n",
      "Testing...\n",
      "Test accuracy epoch 31: 50.07 %\n",
      "[33,   500] loss: 1.999\n",
      "Testing...\n",
      "Test accuracy epoch 32: 49.84 %\n",
      "[34,   500] loss: 1.995\n",
      "Testing...\n",
      "Test accuracy epoch 33: 49.66 %\n",
      "[35,   500] loss: 1.994\n",
      "Testing...\n",
      "Test accuracy epoch 34: 50.19 %\n",
      "[36,   500] loss: 1.990\n",
      "Testing...\n",
      "Test accuracy epoch 35: 50.32 %\n",
      "[37,   500] loss: 1.985\n",
      "Testing...\n",
      "Test accuracy epoch 36: 50.61 %\n",
      "[38,   500] loss: 1.982\n",
      "Testing...\n",
      "Test accuracy epoch 37: 49.24 %\n",
      "[39,   500] loss: 1.982\n",
      "Testing...\n",
      "Test accuracy epoch 38: 50.55 %\n",
      "[40,   500] loss: 1.979\n",
      "Testing...\n",
      "Test accuracy epoch 39: 51.68 %\n",
      "[41,   500] loss: 1.972\n",
      "Testing...\n",
      "Test accuracy epoch 40: 50.78 %\n",
      "[42,   500] loss: 1.970\n",
      "Testing...\n",
      "Test accuracy epoch 41: 50.49 %\n",
      "[43,   500] loss: 1.968\n",
      "Testing...\n",
      "Test accuracy epoch 42: 51.47 %\n",
      "[44,   500] loss: 1.966\n",
      "Testing...\n",
      "Test accuracy epoch 43: 50.79 %\n",
      "[45,   500] loss: 1.962\n",
      "Testing...\n",
      "Test accuracy epoch 44: 50.73 %\n",
      "[46,   500] loss: 1.958\n",
      "Testing...\n",
      "Test accuracy epoch 45: 51.77 %\n",
      "[47,   500] loss: 1.958\n",
      "Testing...\n",
      "Test accuracy epoch 46: 51.32 %\n",
      "[48,   500] loss: 1.952\n",
      "Testing...\n",
      "Test accuracy epoch 47: 51.41 %\n",
      "[49,   500] loss: 1.951\n",
      "Testing...\n",
      "Test accuracy epoch 48: 51.31 %\n",
      "[50,   500] loss: 1.951\n",
      "Testing...\n",
      "Test accuracy epoch 49: 51.36 %\n",
      "[51,   500] loss: 1.945\n",
      "Testing...\n",
      "Test accuracy epoch 50: 51.31 %\n",
      "[52,   500] loss: 1.943\n",
      "Testing...\n",
      "Test accuracy epoch 51: 51.79 %\n",
      "[53,   500] loss: 1.940\n",
      "Testing...\n",
      "Test accuracy epoch 52: 51.32 %\n",
      "[54,   500] loss: 1.938\n",
      "Testing...\n",
      "Test accuracy epoch 53: 51.53 %\n",
      "[55,   500] loss: 1.934\n",
      "Testing...\n",
      "Test accuracy epoch 54: 51.2 %\n",
      "[56,   500] loss: 1.933\n",
      "Testing...\n",
      "Test accuracy epoch 55: 51.77 %\n",
      "[57,   500] loss: 1.928\n",
      "Testing...\n",
      "Test accuracy epoch 56: 50.74 %\n",
      "[58,   500] loss: 1.926\n",
      "Testing...\n",
      "Test accuracy epoch 57: 51.12 %\n",
      "[59,   500] loss: 1.926\n",
      "Testing...\n",
      "Test accuracy epoch 58: 51.42 %\n",
      "[60,   500] loss: 1.921\n",
      "Testing...\n",
      "Test accuracy epoch 59: 50.92 %\n",
      "eta decreased to  0.001\n",
      "[61,   500] loss: 1.909\n",
      "Testing...\n",
      "Test accuracy epoch 60: 52.45 %\n",
      "[62,   500] loss: 1.900\n",
      "Testing...\n",
      "Test accuracy epoch 61: 52.58 %\n",
      "[63,   500] loss: 1.900\n",
      "Testing...\n",
      "Test accuracy epoch 62: 52.84 %\n",
      "[64,   500] loss: 1.898\n",
      "Testing...\n",
      "Test accuracy epoch 63: 52.71 %\n",
      "[65,   500] loss: 1.898\n",
      "Testing...\n",
      "Test accuracy epoch 64: 52.51 %\n",
      "[66,   500] loss: 1.899\n",
      "Testing...\n",
      "Test accuracy epoch 65: 52.56 %\n",
      "[67,   500] loss: 1.898\n",
      "Testing...\n",
      "Test accuracy epoch 66: 52.66 %\n",
      "[68,   500] loss: 1.898\n",
      "Testing...\n",
      "Test accuracy epoch 67: 52.67 %\n",
      "[69,   500] loss: 1.896\n",
      "Testing...\n",
      "Test accuracy epoch 68: 52.56 %\n",
      "[70,   500] loss: 1.894\n",
      "Testing...\n",
      "Test accuracy epoch 69: 52.57 %\n",
      "[71,   500] loss: 1.898\n",
      "Testing...\n",
      "Test accuracy epoch 70: 52.69 %\n",
      "[72,   500] loss: 1.896\n",
      "Testing...\n",
      "Test accuracy epoch 71: 52.51 %\n",
      "[73,   500] loss: 1.896\n",
      "Testing...\n",
      "Test accuracy epoch 72: 52.42 %\n",
      "[74,   500] loss: 1.896\n",
      "Testing...\n",
      "Test accuracy epoch 73: 52.52 %\n",
      "[75,   500] loss: 1.895\n",
      "Testing...\n",
      "Test accuracy epoch 74: 52.9 %\n",
      "[76,   500] loss: 1.893\n",
      "Testing...\n",
      "Test accuracy epoch 75: 52.28 %\n",
      "[77,   500] loss: 1.897\n",
      "Testing...\n",
      "Test accuracy epoch 76: 52.47 %\n",
      "[78,   500] loss: 1.892\n",
      "Testing...\n",
      "Test accuracy epoch 77: 52.57 %\n",
      "[79,   500] loss: 1.893\n",
      "Testing...\n",
      "Test accuracy epoch 78: 52.29 %\n",
      "[80,   500] loss: 1.894\n",
      "Testing...\n",
      "Test accuracy epoch 79: 52.93 %\n",
      "[81,   500] loss: 1.892\n",
      "Testing...\n",
      "Test accuracy epoch 80: 52.64 %\n",
      "[82,   500] loss: 1.891\n",
      "Testing...\n",
      "Test accuracy epoch 81: 52.73 %\n",
      "[83,   500] loss: 1.892\n",
      "Testing...\n",
      "Test accuracy epoch 82: 52.37 %\n",
      "[84,   500] loss: 1.893\n",
      "Testing...\n",
      "Test accuracy epoch 83: 52.49 %\n",
      "[85,   500] loss: 1.892\n",
      "Testing...\n",
      "Test accuracy epoch 84: 52.46 %\n",
      "[86,   500] loss: 1.891\n",
      "Testing...\n",
      "Test accuracy epoch 85: 52.36 %\n",
      "[87,   500] loss: 1.892\n",
      "Testing...\n",
      "Test accuracy epoch 86: 52.43 %\n",
      "[88,   500] loss: 1.893\n",
      "Testing...\n",
      "Test accuracy epoch 87: 52.32 %\n",
      "[89,   500] loss: 1.891\n",
      "Testing...\n",
      "Test accuracy epoch 88: 52.21 %\n",
      "[90,   500] loss: 1.890\n",
      "Testing...\n",
      "Test accuracy epoch 89: 52.37 %\n",
      "eta decreased to  0.0001\n",
      "[91,   500] loss: 1.888\n",
      "Testing...\n",
      "Test accuracy epoch 90: 52.45 %\n",
      "[92,   500] loss: 1.889\n",
      "Testing...\n",
      "Test accuracy epoch 91: 52.43 %\n",
      "[93,   500] loss: 1.890\n",
      "Testing...\n",
      "Test accuracy epoch 92: 52.79 %\n",
      "[94,   500] loss: 1.889\n",
      "Testing...\n",
      "Test accuracy epoch 93: 52.58 %\n",
      "[95,   500] loss: 1.887\n",
      "Testing...\n",
      "Test accuracy epoch 94: 52.62 %\n",
      "[96,   500] loss: 1.888\n",
      "Testing...\n",
      "Test accuracy epoch 95: 52.59 %\n",
      "[97,   500] loss: 1.889\n",
      "Testing...\n",
      "Test accuracy epoch 96: 52.7 %\n",
      "[98,   500] loss: 1.886\n",
      "Testing...\n",
      "Test accuracy epoch 97: 52.57 %\n",
      "[99,   500] loss: 1.890\n",
      "Testing...\n",
      "Test accuracy epoch 98: 52.56 %\n",
      "[100,   500] loss: 1.888\n",
      "Testing...\n",
      "Test accuracy epoch 99: 52.58 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "## learning rate\n",
    "eta = 0.01  \n",
    "## dropout keep rate\n",
    "keep_rate = 0.9\n",
    "## loss --> used to monitor performance, but not for parameter updates (PEPITA does not backpropagate the loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "## optimizer (choose 'SGD' o 'mom')\n",
    "optim = 'mom' # --> default in the paper\n",
    "if optim == 'SGD':\n",
    "    gamma = 0\n",
    "elif optim == 'mom':\n",
    "    gamma = 0.9\n",
    "## batch size\n",
    "batch_size = 64 # --> default in the paper\n",
    "\n",
    "# initialize the network\n",
    "net = NetFC1x1024DOcust()\n",
    "\n",
    "# load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]) # this normalizes to [0,1]\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# define function to register the activations --> we need this to compare the activations in the two forward passes\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "for name, layer in net.named_modules():\n",
    "    layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "\n",
    "# define B --> this is the F projection matrix in the paper (here named B because F is torch.nn.functional)\n",
    "nin = 32*32*3\n",
    "sd = np.sqrt(6/nin)\n",
    "B = (torch.rand(nin,10)*2*sd-sd)*0.05  # B is initialized with the He uniform initialization (like the forward weights)\n",
    "\n",
    "\n",
    "# check cosine similarity before training AND matrix norm\n",
    "angles = []\n",
    "w_all = []\n",
    "norm_w0 = []\n",
    "for l_idx,w in enumerate(net.parameters()):\n",
    "    with torch.no_grad():\n",
    "        w_all.append(copy.deepcopy(w))\n",
    "        if l_idx == 0:\n",
    "            norm_w0.append(torch.norm(w))\n",
    "        print('norm of w at layer {} is {}'.format(l_idx,torch.norm(w)))\n",
    "w_prod = w_all[0].T\n",
    "for idx in range(1,len(w_all)):\n",
    "    w_prod = torch.matmul(w_prod,w_all[idx].T)\n",
    "    print(w_prod.size())\n",
    "\n",
    "# do one forward pass to get the activation size needed for setting up the dropout masks\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = torch.flatten(images, 1) # flatten all dimensions except batch        \n",
    "outputs = net(images,do_masks=None)\n",
    "layers_act = []\n",
    "for key in activation:\n",
    "    if 'fc' in key or 'conv' in key:\n",
    "        layers_act.append(F.relu(activation[key]))\n",
    "        \n",
    "# set up for momentum\n",
    "if optim == 'mom':\n",
    "    gamma = 0.9\n",
    "    v_w_all = []\n",
    "    for l_idx,w in enumerate(net.parameters()):\n",
    "        if len(w.shape)>1:\n",
    "            with torch.no_grad():\n",
    "                v_w_all.append(torch.zeros(w.shape))\n",
    "\n",
    "# Train and test the model\n",
    "test_accs = []\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    # learning rate decay\n",
    "    if epoch in [60,90]: \n",
    "        eta = eta*0.1\n",
    "        print('eta decreased to ',eta)\n",
    "    \n",
    "    # loop over batches\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, target = data\n",
    "        inputs = torch.flatten(inputs, 1) # flatten all dimensions except batch\n",
    "        target_onehot = F.one_hot(target,num_classes=10)\n",
    "        \n",
    "        # create dropout mask for the two forward passes --> we need to use the same mask for the two passes\n",
    "        do_masks = []\n",
    "        if keep_rate < 1:\n",
    "            for l in layers_act[:-1]:\n",
    "                input1 = l\n",
    "                do_mask = Variable(torch.ones(inputs.shape[0],input1.data.new(input1.data.size()).shape[1]).bernoulli_(keep_rate))/keep_rate\n",
    "                do_masks.append(do_mask)\n",
    "            do_masks.append(1) # for the last layer we don't use dropout --> just set a scalar 1 (needed for when we register activation layer)\n",
    "     \n",
    "        # forward pass 1 with original input --> keep track of activations\n",
    "        outputs = net(inputs,do_masks)\n",
    "        layers_act = []\n",
    "        cnt_act = 0\n",
    "        for key in activation:\n",
    "            if 'fc' in key or 'conv' in key:\n",
    "                layers_act.append(F.relu(activation[key])* do_masks[cnt_act]) # Note: we need to register the activations taking into account non-linearity and dropout mask\n",
    "                cnt_act += 1\n",
    "                \n",
    "        # compute the error\n",
    "        error = outputs - target_onehot  \n",
    "        \n",
    "        # modify the input with the error\n",
    "        error_input = error @ B.T\n",
    "        mod_inputs = inputs + error_input\n",
    "        \n",
    "        # forward pass 2 with modified input --> keep track of modulated activations\n",
    "        mod_outputs = net(mod_inputs,do_masks)\n",
    "        mod_layers_act = []\n",
    "        cnt_act = 0\n",
    "        for key in activation:\n",
    "            if 'fc' in key or 'conv' in key:\n",
    "                mod_layers_act.append(F.relu(activation[key])* do_masks[cnt_act]) # Note: we need to register the activations taking into account non-linearity and dropout mask\n",
    "                cnt_act += 1\n",
    "        mod_error = mod_outputs - target_onehot\n",
    "        \n",
    "        # compute the delta_w for the batch\n",
    "        delta_w_all = []\n",
    "        v_w = []\n",
    "        for l_idx,w in enumerate(net.parameters()):\n",
    "            v_w.append(torch.zeros(w.shape))\n",
    "            \n",
    "        for l in range(len(layers_act)):\n",
    "            \n",
    "            # update for the last layer\n",
    "            if l == len(layers_act)-1:\n",
    "                \n",
    "                if len(layers_act)>1:\n",
    "                    delta_w = -mod_error.T @ mod_layers_act[-2]\n",
    "                else:\n",
    "                    delta_w = -mod_error.T @ mod_inputs\n",
    "            \n",
    "            # update for the first layer\n",
    "            elif l == 0:\n",
    "                delta_w = -(layers_act[l] - mod_layers_act[l]).T @ mod_inputs\n",
    "            \n",
    "            # update for the hidden layers (not first, not last)\n",
    "            elif l>0 and l<len(layers_act)-1:\n",
    "                delta_w = -(layers_act[l] - mod_layers_act[l]).T @ mod_layers_act[l-1]\n",
    "            \n",
    "            delta_w_all.append(delta_w)\n",
    "                \n",
    "        # apply the weight change\n",
    "        if optim == 'SGD':\n",
    "            for l_idx,w in enumerate(net.parameters()):\n",
    "                with torch.no_grad():\n",
    "                    w += eta * delta_w_all[l_idx]/batch_size # specify for which layer\n",
    "                    \n",
    "        elif optim == 'mom':\n",
    "            for l_idx,w in enumerate(net.parameters()):\n",
    "                with torch.no_grad():\n",
    "                    v_w_all[l_idx] = gamma * v_w_all[l_idx] + eta * delta_w_all[l_idx]/batch_size\n",
    "                    w += v_w_all[l_idx]\n",
    "                    \n",
    "        \n",
    "        # keep track of the loss\n",
    "        loss = criterion(outputs, target)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i%500 == 499:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "                     \n",
    "    print('Testing...')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for test_data in testloader:\n",
    "            test_images, test_labels = test_data\n",
    "            test_images = torch.flatten(test_images, 1) # flatten all dimensions except batch\n",
    "            # calculate outputs by running images through the network\n",
    "            test_outputs = net(test_images,do_masks=None)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            total += test_labels.size(0)\n",
    "            correct += (predicted == test_labels).sum().item()\n",
    "\n",
    "    print('Test accuracy epoch {}: {} %'.format(epoch, 100 * correct / total))\n",
    "    test_accs.append(100 * correct / total)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc18db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ced41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21844ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
